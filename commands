
Enter the following command to update the system's apt package manager index and update packages required to install Docker:


# Update the package index
sudo apt-get update
# Update packages required for HTTPS package repository access
sudo apt-get install -y apt-transport-https ca-certificates curl software-properties-common
 

2. Install Docker community edition using Ubuntu's apt package manager and the official Docker repository:

# Add Dockerâ€™s GPG key
curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -
# Configure the stable Docker release repository
sudo add-apt-repository \
 "deb [arch=amd64] https://download.docker.com/linux/ubuntu \
 $(lsb_release -cs) \
 stable"
# Update the package index to include the stable Docker repository
sudo apt-get update
# Install Docker
sudo apt-get install -y docker-ce=17.03.2~ce-0~ubuntu-xenial
The installation instructions are based on Docker's official instructions. Although it is possible to install Ubuntu's docker.io package, to control the version and use a version that is officially supported by Kubernetes, Docker community edition 17.03 is installed directly from Docker's package repository.

3. Confirm Docker 17.03 is installed:
docker --version

4. Install kubeadm, kubectl, and kubelet from the official Kubernetes package repository:

# Add the Google Cloud packages GPG key
curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -
# Add the Kubernetes release repository
sudo add-apt-repository "deb http://apt.kubernetes.io/ kubernetes-xenial main"
# Update the package index to include the Kubernetes repository
sudo apt-get update
# Install the packages
sudo apt-get install -y kubeadm=1.13.4-00 kubelet=1.13.4-00 kubectl=1.13.4-00 kubernetes-cni=0.6.0-00
# Prevent automatic updates to the installed packages
sudo apt-mark hold kubelet kubeadm kubectl
The version of all the packages is set to 1.13.4 for consistency in Lab experiences, and so that you can perform a cluster upgrade in a later Lab Step.

 

5. Display the help page for kubeadm:


kubeadm

 . Initialize the master node using the init command:


sudo kubeadm init --pod-network-cidr=192.168.0.0/16 --kubernetes-version=stable-1.13
The pod network CIDR block (192.168.0.0/16) is the default used by Weave. The CIDR does not overlap with the Amazon VPC network CIDR . If it did, you would need to perform additional configuration of Weave later on to avoid the overlap. The output reports the steps that kubeadm takes to initialize the master:

 

2. Copy the kubeadm join command at the end of the output and store it somewhere so that we can access later.
 

3. Initialize  user's default kubectl configuration using the admin kubeconfig file generated by kubeadm:

mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
 

4. Confirm you can use kubectl to get the cluster component statuses:


kubectl get component statuses


The output confirms that the scheduler, controller-manager, and etcd are all Healthy. The Kubernetes API server is also operational, or kubectl would have returned an error attempting to connect to the API server. Enter kubeadm token --help if you would like to know more about kubeadm tokens.

 

5. Get the nodes in the cluster:

kubectl get nodes


The master node is reporting a STATUS of NotReady. Notice kubeadm gives the node a NAME based on its IP address. The --node-name option can be used to override the default behavior.

 

6. Describe the node to probe deeper into its NotReady status:


kubectl describe nodes
In the Conditions section of the output, observe the Ready condition is False, and read the Message:



The kubelet is not ready because the network plugin is not ready. The cni config uninitialized refers to the container network interface (CNI) and is a related problem. Network plugins implement the CNI interface.  will resolve the issue by initializing the Weave network plugin.

 

7. Enter the following commands to install the Weave pod network plugin:


kubectl apply -f https://docs.projectcalico.org/v3.1/getting-started/kubernetes/installation/hosted/rbac-kdd.yaml
kubectl apply -f "https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d '\n')&env.IPALLOC_RANGE=192.168.0.0/16"
alt

The commands first install the cluster roles and bindings that are used by Weave (rbac-kdd.yaml). Then a variety of resources are created to support pod networking. A daemonset is used to run a Weave-node pod on each node in the cluster. The resources include several custom resources (customresourcedefinition) that extend the Kubernetes API, for example, to support network policies (networkpolicies.crd.projectWeave.org). Many network plugins have a similar installation procedure.

 

8. Check the status of the nodes in the cluster:


kubectl get nodes


With the network plugin initialized, the master node is now Ready.

. Open a second terminal connected to instance-b that is listed in the EC2 Console.

Refer back to the earlier  Step on connecting to instance-a using EC2 Instance Connect, if required. The SSH user name is again ubuntu.

 

2. Enter sudo followed by the kubeadm join command that you stored from the output of kubeadm init. It resembles:


sudo kubeadm join 10.0.0.###:6443 --token ... --discovery-token-ca-cert-hash sha256:...


Read through the output to understand the operations kubeadm performed.

 

3. In the master node's SSH shell, confirm the worker node is part of the cluster:

kubectl get nodes


The worker node appears with a role of <none>.

 

4. Confirm that all the pods in the cluster are running:


kubectl get pods --all-namespaces


All of the pods are Running, and the two-node cluster is operational. Notice that there are two weave-node pods that support pod networking on each node.

 1. In the master node SSH shell, create a deployment of the Nginx application with two replicas:


kubectl create deployment nginx --image=nginx
kubectl scale deployment nginx --replicas=2


The deployment and the following service are simply created so that you can confirm they exist after performing a cluster restore operation at the end of  Step.

 

2. Expose the deployment using a ClusterIP service:


kubectl expose deployment nginx --type=ClusterIP --port=80 --target-port=80 --name=web


 

3. Send a HTTP request to the web service:

# Get the Cluster IP of the service
service_ip=$(kubectl get service web -o jsonpath='{.spec.clusterIP}')
# Use curl to send an HTTP request to the service
curl $service_ip


The Nginx server response verifies that everything is working as expected.

 

4. Create a management namespace:

kubectl create namespace management 


 

5. Create a job that creates a pod, and issues the etcdctl snapshot save command to back up the cluster:

cat <<EOF | kubectl create -f -
apiVersion: batch/v1
kind: Job
metadata:
  name: backup
  namespace: management
spec:
  template:
    spec:
      containers:
      # Use etcdctl snapshot save to create a snapshot in the /snapshot directory 
      - command:
        - /bin/sh 
        args:
        - -ec
        - etcdctl --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/peer.crt --key=/etc/kubernetes/pki/etcd/peer.key snapshot save /snapshots/backup.db
        # The same image used by the etcd pod
        image: k8s.gcr.io/etcd-amd64:3.1.12
        name: etcdctl
        env:
        # Set the etcdctl API version to 3 (to match the version of etcd installed by kubeadm)
        - name: ETCDCTL_API
          value: '3'
        volumeMounts:
        - mountPath: /etc/kubernetes/pki/etcd
          name: etcd-certs
          readOnly: true
        - mountPath: /snapshots
          name: snapshots
      # Use the host network where the etcd port is accessible (etcd pod uses host network)
      # This allows the etcdctl to connect to etcd that is listening on the host network
      hostNetwork: true
      affinity:
        # Use node affinity to schedule the pod on the master (where the etcd pod is)
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: node-role.kubernetes.io/master
                operator: Exists
      restartPolicy: OnFailure
      tolerations:
      # tolerate the master's NoSchedule taint to allow scheduling on the master
      - effect: NoSchedule
        operator: Exists
      volumes:
      # Volume storing the etcd PKI keys and certificates
      - hostPath:
          path: /etc/kubernetes/pki/etcd
          type: DirectoryOrCreate
        name: etcd-certs
      # A volume to store the backup snapshot
      - hostPath:
          path: /snapshots
          type: DirectoryOrCreate
        name: snapshots
EOF
You could also create a CronJob Kubernetes resource instead of a one-off Job to periodically perform the backup operation. A Job is sufficient for this Lab. Read through the Job manifest, and use the comments to help understand what it does.

The etcdctl command (see spec.template.spec.containers.args) requires the certificate authority certificate, a client key, and a client certificate to encrypt the etcd traffic. kubeadm configures etcd to listen to HTTPS only as a security best practice. The snapshot save command creates a snapshot of the entire key-value store at the given location (/snapshots/backup.db).

 

6. List the contents of the /snapshots directory:


ls /snapshots


The etcd snapshot saved by the pod is present. You will now cause the master to fail and remove the data files of the etcd key-value store to simulate a substantial cluster failure.

 

7. Stop the master's kubelet:


sudo systemctl stop kubelet.service
The kubelet will automatically try to restart the etcd pod if it detects that it has been deleted qne need to stop the kubelet to prevent this.

 

8. Delete the etcd containers in Docker that are created by the Kubernetes etcd pod:

sudo docker ps | grep etcd | cut -d' ' -f1 | xargs sudo docker rm -f


The container IDs of the deleted etcd containers are displayed.

 

9. Delete the etcd data files persisted to disk:


sudo rm -rf /var/lib/etcd/*
The etcd pod mounts /var/lib/etcd to persist its data to disk.

 

10. Use a Docker container to restore the /var/lib/etcd data from the backup snapshot:


sudo docker run --rm \
    -v '/snapshots:/snapshots' \
    -v '/var/lib/etcd:/var/lib/etcd' \
    -e ETCDCTL_API=3 \
    'k8s.gcr.io/etcd-amd64:3.1.12' \
    /bin/sh -c "etcdctl snapshot restore '/snapshots/backup.db' && mv /default.etcd/member /var/lib/etcd"
alt

You need to directly use Docker instead of creating a pod in Kubernetes because Kubernetes will not function with the kubelet and etcd offline. The kubelet will recreate the etcd pod from the static pod manifest in /etc/kubernetes/manifests/etcd.yaml. The etcdctl snapshot restore command performs the restore operation.

 

11. Start the kubelet: 


sudo systemctl start kubelet
The kubelet automatically recreates the missing etcd pod containers. The pod will use the restored data files created from the backup, and Kubernetes will have a restored view of the cluster.

 

12. Confirm the Nginx pods are running:


kubectl get pods


 

13. Confirm the web service works:

service_ip=$(kubectl get service web -o jsonpath='{.spec.clusterIP}')
curl $service_ip 
 

 Download version 1.14.1 of kubeadm:

# Update the kubeadm binary with version 1.14.1
sudo curl -sSL https://dl.k8s.io/release/v1.14.1/bin/linux/amd64/kubeadm -o /usr/bin/kubeadm
Currently, you cannot upgrade kubeadm using the apt package manager until after upgrading the control plane. This limitation should be removed in future versions of kubeadm.

 

2. Generate an upgrade plan for upgrading Kubernetes to version 1.14.1:


sudo kubeadm upgrade plan v1.14.1


As the output explains, several checks are performed, and the requirements for upgrading the cluster are first verified. A reminder that you need to manually upgrade the kubelet on each node in the cluster is then displayed. Future versions may remove this manual step. Finally, a summary of the planned version changes for all the cluster components (COMPONENT) is presented.

 

3. Apply the upgrade plan by issuing the following command and entering y when prompted:


sudo kubeadm upgrade apply v1.14.1
kubeadm begins upgrading the cluster components on the master node. Read through the output to understand what steps are being performed. It takes approximately four minutes to complete. You will see the following success message to know everything went as expected:



Note: If the upgrade procedure times out, you can safely try again until it succeeds. The upgrade command is idempotent so you can run the command as many times as required to complete the upgrade. This issue is being worked on and should be resolved in future versions of kubeadm.

 

4. Prepare to upgrade the master node's kubelet by draining the node:


kubectl drain $HOSTNAME --ignore-daemonsets

 

5. Upgrade the kubelet, kubeadm, and kubectl apt packages:


sudo apt-get update
sudo apt-get upgrade -y --allow-change-held-packages \
     kubelet=1.14.1-00 kubeadm=1.14.1-00 kubectl=1.14.1-00 kubernetes-cni=0.7.5-00
The upgrade may take a few minutes to complete.

Note: If  see a Configuring grub-pc menu, select Keep the local version currently installed:



Press space to select the first device, followed by enter to press the <Ok> button:


 

6. Uncordon the master to allow pods to be scheduled on it now that is has been upgraded:

kubectl uncordon $HOSTNAME
 

7. Get the node information to confirm that the version of the master is 1.14.1:

kubectl get nodes

 

8. Drain the worker node to prepare it for upgrading:

Copy code
# Get the worker's name
worker_name=$(kubectl get nodes | grep \<none\> | cut -d' ' -f1)
# Drain the worker node
kubectl drain $worker_name --ignore-daemonsets
alt

 

9. In the SSH shell connected to the worker node, drain the node and upgrade the Kubernetes packages:

Copy code
sudo apt-get update
sudo apt-get upgrade -y --allow-change-held-packages \
     kubelet=1.14.1-00 kubeadm=1.14.1-00 kubectl=1.14.1-00 kubernetes-cni=0.7.5-00
  

10. Upgrade the worker node's kubelet config using kubeadm:

Copy code
sudo kubeadm upgrade node config --kubelet-version v1.14.1
alt

 

11. Restart the worker node's kubelet:

Copy code
sudo systemctl restart kubelet
 

12. Return to the master's SSH shell and uncordon the worker node:

Copy code
kubectl uncordon $worker_name
alt

 

13. Confirm the worker node is ready and running version 1.14.1:

Copy code
kubectl get nodes
alt

The upgrade process is now complete. You can create some pods if you want to further test the upgrade succeeded.

 
